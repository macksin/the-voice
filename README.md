Got it. I’ll research the cost of training and inference for multiple LLM sizes using A10G, A100, H100, Inferentia 2, and Trainium 2 GPUs/accelerators, at both full and half precision. I’ll look at pricing from major cloud providers like AWS, GCP, and Azure, and where available, include comparisons of throughput (e.g., tokens/sec or TFLOPs), energy usage, and cost-efficiency per model size.

I’ll let you know as soon as the breakdown is ready.

# Cost & Performance of LLM Training and Inference on Various Accelerators

Large Language Models (LLMs) at scales from 1B to 70B parameters have significant computational demands. This report compares the **cost and performance** of training and inference across multiple model sizes on five accelerator platforms: **NVIDIA A10G, NVIDIA A100, NVIDIA H100, AWS Inferentia2, and AWS Trainium**. We consider both full precision (FP32) and half precision (FP16/BF16) scenarios, and highlight cloud pricing (AWS, GCP, Azure), performance metrics (TFLOPs, throughput in tokens/sec), estimated training costs, inference cost per token, efficiency metrics (cost per TFLOP and per million tokens), and hardware precision support differences.

## Cloud Pricing for Each Accelerator

**On-demand cloud pricing** varies by provider and instance type. The table below summarizes typical hourly rates on AWS, Google Cloud (GCP), and Azure for instances featuring these accelerators (prices are for Linux on-demand in a reference region, per GPU or accelerator chip):

| **Accelerator**       | **AWS (EC2)**                        | **GCP (Compute Engine)**             | **Azure (VM)**                        |
|-----------------------|--------------------------------------|--------------------------------------|---------------------------------------|
| **NVIDIA A10G** (24GB)  | ~$1.00 per hour (1 x A10G on G5) ([g5.xlarge pricing and specs - Vantage](https://instances.vantage.sh/aws/ec2/g5.xlarge#:~:text=Size%20vCPUs%20Memory%20%28GiB%29%20g5,48xlarge%20192%20768)) ([g5.xlarge pricing and specs - Vantage](https://instances.vantage.sh/aws/ec2/g5.xlarge#:~:text=GPU%201%20GPU%20Architecture%20NVIDIA,Enhanced%20Networking%20True%20IPV6%20True)) | *Limited (not offered as standalone GPU in GCP)* | ~$0.91 per hour (1/1 of A10 in NVads A10 v5) ([NV12ads A10 v5 pricing and specs - Vantage](https://instances.vantage.sh/azure/vm/nv12ads-v5#:~:text=NV12ads%20A10%20v5)) |
| **NVIDIA A100** 40GB   | ~$4.10 per hour per GPU (e.g. 8×A100 40GB = $32.77/hr on AWS p4d.24xl) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=The%20H100%20%20is%20currently,for%20a%208x%20GPU%20instance)) | ~$4.05 per hour (A100 40GB on GCP) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=a2)) | *80GB variant on Azure:* ~$3.67 per hour (A100 80GB) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=NC24ads%20A100%20v4)) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=%2412)) |
| **NVIDIA A100** 80GB   | *Not offered on AWS in P4 generation* ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=The%20H100%20%20is%20currently,for%20a%208x%20GPU%20instance)) | ~$6.25 per hour (A100 80GB on GCP) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=a2)) | ~$3.67 per hour (A100 80GB on Azure NCv4) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=NC24ads%20A100%20v4)) |
| **NVIDIA H100** 80GB   | ~$12.29 per hour per GPU (e.g. 8×H100 = ~$98.32/hr on AWS p5.48xl) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=%2424)) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=The%20H100%20%20is%20currently,for%20a%208x%20GPU%20instance)) | ~$11.06 per hour (H100 80GB on GCP) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=a3)) | ~$6.98 per hour (H100 80GB on Azure NCv5) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=NC40ads%20H100%20v5)) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=%2412)) |
| **AWS Inferentia2** (Inf2) | ~$1.08 per chip-hour (e.g. 12-chip Inf2.48xl = $12.98/hr on AWS) ([inf2.48xlarge pricing and specs - Vantage](https://instances.vantage.sh/aws/ec2/inf2.48xlarge#:~:text=inf2)). Smaller Inf2 instances down to ~$0.758/hr for 1 chip ([inf2.xlarge pricing and specs - Amazon EC2 Instance Comparison](https://instances.vantage.sh/aws/ec2/inf2.xlarge#:~:text=inf2.xlarge%20pricing%20and%20specs%20,7582)). | *N/A (AWS only)* | *N/A (AWS only)* |
| **AWS Trainium** (Trn1) | ~$1.35 per chip-hour (e.g. 16-chip Trn1.32xl = $21.5/hr on AWS) ([Distributed Training of Large Language Models on AWS Trainium](https://assets.amazon.science/fa/fc/6c9a63824f1fa3655fc757825256/distributed-training-of-large-language-models-on-aws-trainium.pdf#:~:text=BF16%2FFP16%20TFLOPS%20190%20312%20Onboard,accelerator%20%28GB%2Fsec%2Faccelerator%29%20384%20600)). | *N/A (AWS only)* | *N/A (AWS only)* |

**Notes:** Pricing can vary by region and commitment. Azure’s NVads A10 v5 series allows fractional A10G usage (down to 1/6th of a GPU) for lower cost of entry ([NVIDIA A10-Powered Instances From Azure Deliver Accelerated Graphics and Computing in the Cloud | NVIDIA Blog](https://blogs.nvidia.com/blog/a10-azure-instances/#:~:text=The%20NVIDIA%20A10%20GPU%2C%20which,for%20larger%2C%20more%20complex%20workloads)). AWS’s Trainium (Trn1) instances are priced about one-third lower than comparable A100 GPU instances for similar workloads ([Distributed Training of Large Language Models on AWS Trainium](https://assets.amazon.science/fa/fc/6c9a63824f1fa3655fc757825256/distributed-training-of-large-language-models-on-aws-trainium.pdf#:~:text=A100%20GPU%20,3040%20BF16%2FFP16%20TFLOPS)) (p4d at $32.77/hr vs trn1 at $21.5/hr). AWS’s new H100-based P5 instances carry a premium (on-demand ~$98/hr for 8×H100) reflecting their cutting-edge performance ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=The%20H100%20%20is%20currently,for%20a%208x%20GPU%20instance)). Google and Azure also offer H100s at lower per-GPU rates (Azure ~$6.98/hr, GCP ~$11/hr) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=NC40ads%20H100%20v5)) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=a3)), which may reflect differing regional pricing or promotional pricing on those platforms.

## Peak Compute Performance and Precision Support

**Theoretical peak performance** of each accelerator is compared below (per chip/GPU):

- **NVIDIA A10G** (Ampere architecture): ~31.2 TFLOPS FP32; ~125 TFLOPS FP16/BF16 on tensor cores (250 TFLOPS with sparsity); ~250 INT8 TOPS ([NVIDIA A10 Tensor Core GPU](https://www.nvidia.com/en-us/data-center/products/a10-gpu/#:~:text=A10%20Technical%20Specifications%20and%20Features,250)). 24 GB GDDR6 memory, 600 GB/s bandwidth ([[PDF] NVIDIA A10 datasheet](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a10/pdf/a10-datasheet.pdf#:~:text=,vGPU)). No native FP8 support. Designed primarily for **inference and moderate training**, it supports FP32, FP16, BF16, and INT8 arithmetic (no TensorFloat-32 or FP8). Its 150W TDP yields a high efficiency (~0.008 $/TFLOP-hour on AWS).

- **NVIDIA A100** (Ampere, 40GB or 80GB): 19.5 TFLOPS FP32 (non-tensor) ([Theoretical TFLOPS for FP16, BF16 and TF32 for tensor and non-tensor - GPU-Accelerated Libraries - NVIDIA Developer Forums](https://forums.developer.nvidia.com/t/theoretical-tflops-for-fp16-bf16-and-tf32-for-tensor-and-non-tensor/218102#:~:text=,Tensor%29%20%7C19.5%7C60%7C48)); up to 156 TFLOPS using TF32 tensor cores ([Theoretical TFLOPS for FP16, BF16 and TF32 for tensor and non-tensor - GPU-Accelerated Libraries - NVIDIA Developer Forums](https://forums.developer.nvidia.com/t/theoretical-tflops-for-fp16-bf16-and-tf32-for-tensor-and-non-tensor/218102#:~:text=GPU%20Features%20NVIDIA%20A100%20NVIDIA,Tensor%29%20%7C78%7C120%7C96)); 312 TFLOPS FP16/BF16 tensor (no sparsity) or 624 with sparsity ([Theoretical TFLOPS for FP16, BF16 and TF32 for tensor and non-tensor - GPU-Accelerated Libraries - NVIDIA Developer Forums](https://forums.developer.nvidia.com/t/theoretical-tflops-for-fp16-bf16-and-tf32-for-tensor-and-non-tensor/218102#:~:text=GPU%20Features%20NVIDIA%20A100%20NVIDIA,Tensor%29%20%7C78%7C120%7C96)); up to 624 INT8 TOPS (1248 with sparsity) ([Theoretical TFLOPS for FP16, BF16 and TF32 for tensor and non-tensor - GPU-Accelerated Libraries - NVIDIA Developer Forums](https://forums.developer.nvidia.com/t/theoretical-tflops-for-fp16-bf16-and-tf32-for-tensor-and-non-tensor/218102#:~:text=GPU%20Features%20NVIDIA%20A100%20NVIDIA,Tensor%29%20%7C78%7C120%7C96)). 40 or 80 GB HBM2e memory at 1.6–2.0 TB/s. Supports **mixed precision**: FP32, TF32, FP16, BF16, INT8, etc. A100 was the workhorse for LLM training, with broad framework support and **tensor core acceleration** for FP16/BF16. Cost per TFLOP is moderate (~$0.013 per TFLOP-hour on AWS).

- **NVIDIA H100** (Hopper, 80GB): ~60 TFLOPS FP32; 120 TFLOPS FP16/BF16 (non-tensor) ([Theoretical TFLOPS for FP16, BF16 and TF32 for tensor and non-tensor - GPU-Accelerated Libraries - NVIDIA Developer Forums](https://forums.developer.nvidia.com/t/theoretical-tflops-for-fp16-bf16-and-tf32-for-tensor-and-non-tensor/218102#:~:text=,Tensor%29%20%7C19.5%7C60%7C48)); up to **1000 TFLOPS** FP16/BF16 tensor (no sparsity) or 2000 with sparsity on SXM form factor ([Theoretical TFLOPS for FP16, BF16 and TF32 for tensor and non-tensor - GPU-Accelerated Libraries - NVIDIA Developer Forums](https://forums.developer.nvidia.com/t/theoretical-tflops-for-fp16-bf16-and-tf32-for-tensor-and-non-tensor/218102#:~:text=GPU%20Features%20NVIDIA%20A100%20NVIDIA,Tensor%29%20%7C78%7C120%7C96)) (~800/1600 on PCIe) ([Theoretical TFLOPS for FP16, BF16 and TF32 for tensor and non-tensor - GPU-Accelerated Libraries - NVIDIA Developer Forums](https://forums.developer.nvidia.com/t/theoretical-tflops-for-fp16-bf16-and-tf32-for-tensor-and-non-tensor/218102#:~:text=GPU%20Features%20NVIDIA%20A100%20NVIDIA,Tensor%29%20%7C78%7C120%7C96)). Introduces FP8: up to 1000 TFLOPS FP8 tensor. 80 GB HBM3 at ~3.0 TB/s. Supports new precisions (FP8) in addition to FP32/TF32/FP16/BF16/INT8 ([H100 Tensor Core GPU - NVIDIA](https://www.nvidia.com/en-us/data-center/h100/#:~:text=Fourth,usage%20and%20increase%20performance)). **Significant speedups** for LLMs: H100 can achieve up to 4×–5× the throughput of A100 by using FP8 for transformer workloads ([H100 has 4.6x A100 Performance in TensorRT-LLM, achieving ...](https://nvidia.github.io/TensorRT-LLM/blogs/H100vsA100.html#:~:text=H100%20has%204,1st%20token%20latency%20than%20A100)) ([H100 has 4.6x A100 Performance in TensorRT-LLM, achieving 10,000 tok/s at 100ms to first token — tensorrt_llm  documentation](https://nvidia.github.io/TensorRT-LLM/blogs/H100vsA100.html#:~:text=TensorRT,10ms%20to%201st%20token%20latency)). For example, H100 with FP8 delivered **10,000 tokens/sec** on a 6B GPT-J model, ~4.6× the A100 FP16 throughput (≈2,370 tok/s) ([H100 has 4.6x A100 Performance in TensorRT-LLM, achieving 10,000 tok/s at 100ms to first token — tensorrt_llm  documentation](https://nvidia.github.io/TensorRT-LLM/blogs/H100vsA100.html#:~:text=TensorRT,10ms%20to%201st%20token%20latency)). Despite a higher per-hour cost, the cost-per-compute is similar or better than A100 (roughly $0.012 per TFLOP-hour on AWS) due to massive performance.

- **AWS Inferentia2** (Inf2, 2nd-gen AWS inference ASIC): Each Inferentia2 chip provides up to **190 TFLOPS FP16/BF16** and also supports FP32 and the new configurable FP8 (cFP8) data type ([
    
      High performance Llama 2 deployments with AWS Inferentia2 using TorchServe | PyTorch
    
  ](https://pytorch.org/blog/high-performance-llama/#:~:text=Inferentia2%20supports%20FP32%2C%20TF32%2C%20BF16%2C,removing%20the%20need%20for%20lower)). 32 GB onboard memory per chip, with high-speed NeuronLink interconnect between chips for scaling models. Notably, **Inferentia2 supports FP32 and TF32** for ease of model conversion ([
    
      High performance Llama 2 deployments with AWS Inferentia2 using TorchServe | PyTorch
    
  ](https://pytorch.org/blog/high-performance-llama/#:~:text=Inferentia2%20supports%20FP32%2C%20TF32%2C%20BF16%2C,removing%20the%20need%20for%20lower)), although it’s optimized for lower precision inference. It delivers **4× higher throughput** and **10× lower latency** than first-gen Inferentia (Inf1) ([AWS Inferentia and Trainium for Generative AI: A Comprehensive Guide - CloudThat Resources](https://www.cloudthat.com/resources/blog/aws-inferentia-and-trainium-for-generative-ai-a-comprehensive-guide/#:~:text=AWS%20Inferentia%20is%20designed%20to,LLMs%29%20or%20diffusion%20models)). While not typically used for full FP32 math, it can ingest FP32 models and internally use BF16/INT8 for efficiency. **Precision support limitations:** Inferentia1 lacked FP32, but Inferentia2 adds FP32/TF32 support for compatibility (though FP32 throughput is much lower than FP16). Cost per TFLOP is extremely low (~$0.005–$0.006 per TFLOP-hour on AWS on-demand), making it very cost-efficient for inference.

- **AWS Trainium** (Trn1, 1st-gen AWS training ASIC): Each Trainium chip has 2 NeuronCores, together delivering ~190 BF16/FP16 TFLOPS ([Distributed Training of Large Language Models on AWS Trainium](https://assets.amazon.science/fa/fc/6c9a63824f1fa3655fc757825256/distributed-training-of-large-language-models-on-aws-trainium.pdf#:~:text=match%20at%20L378%20BF16%2FFP16%20TFLOPS,accelerator%20%28GB%2Fsec%2Faccelerator%29%20384%20600)) (with specialized tensor cores ~90 TFLOPS per NeuronCore ([Distributed Training of Large Language Models on AWS Trainium](https://assets.amazon.science/fa/fc/6c9a63824f1fa3655fc757825256/distributed-training-of-large-language-models-on-aws-trainium.pdf#:~:text=%E2%80%A2%20Tensor%20Engine%20is%20based,layer%20normalization%20and%20pooling))). 32 GB HBM per chip, 1.2 TB/s bandwidth ([Google TPUs vs. AWS Trainium & Inferentia vs. NVIDIA GPUs](https://www.ankursnewsletter.com/p/google-tpus-vs-aws-trainium-and-inferentia#:~:text=GPUs%20www,Memory%20bandwidth%3A%201200%20GB%2Fs)). Trainium supports FP32 in a limited capacity (each chip has vector/scalar units totaling ~5.2 TFLOPS FP32 for non-matrix ops) ([Distributed Training of Large Language Models on AWS Trainium](https://assets.amazon.science/fa/fc/6c9a63824f1fa3655fc757825256/distributed-training-of-large-language-models-on-aws-trainium.pdf#:~:text=general%20matrix%20multiply%2C%20convolution%2C%20reshape%2C,3%20TFLOPS%20of%20FP32%20computations)), but is mainly optimized for BF16/FP16 training (and supports INT8 for certain ops). No FP8 support in this generation. **Trainium’s unique advantage** is price-performance: a Trn1 instance with 16 chips offers 3.04 PFLOPS BF16, slightly above an 8×A100 instance’s 2.496 PFLOPS ([Distributed Training of Large Language Models on AWS Trainium](https://assets.amazon.science/fa/fc/6c9a63824f1fa3655fc757825256/distributed-training-of-large-language-models-on-aws-trainium.pdf#:~:text=A100%20GPU%20,3040%20BF16%2FFP16%20TFLOPS)) ([Distributed Training of Large Language Models on AWS Trainium](https://assets.amazon.science/fa/fc/6c9a63824f1fa3655fc757825256/distributed-training-of-large-language-models-on-aws-trainium.pdf#:~:text=BF16%2FFP16%20TFLOPS%20190%20312%20Onboard,accelerator%20%28GB%2Fsec%2Faccelerator%29%20384%20600)). AWS claims up to *50% lower cost-to-train* for Trainium-based instances vs GPU instances ([AWS Inferentia and Trainium for Generative AI: A Comprehensive Guide - CloudThat Resources](https://www.cloudthat.com/resources/blog/aws-inferentia-and-trainium-for-generative-ai-a-comprehensive-guide/#:~:text=Training%20large%20models%2C%20especially%20those,vision%2C%20recommendation%20systems%2C%20and%20more)). Indeed, cost per TFLOP-hour is roughly half that of A100 (~$0.007 per TFLOP-hour). Trainium is purpose-built for training, so unlike Inferentia it doesn’t target ultra-low latency inference (Trn1 is used for training and can also handle large-batch inference or fine-tuning).

**Hardware precision limitations:** All NVIDIA GPUs fully support FP32, FP16, BF16 (A100/H100), and INT8. H100 added FP8 which A100/A10 lack. AWS Trainium and Inferentia embrace **mixed-precision** – for example, BF16 with stochastic rounding is used to avoid the inefficiency of full FP32 training ([HLAT: High-quality Large Language Model Pre-trained on AWS Trainium](https://arxiv.org/html/2404.10630v1#:~:text=overall%20training%20throughput)). Inferentia2 now supports FP32 for compatibility, but heavy FP32 math on these AWS chips is not their strength (they rely on FP16/BF16 for throughput). In summary, **Inferentia1** did *not* support FP32 at all, while **Inferentia2 does** (with autocasting down to lower precision) ([
    
      High performance Llama 2 deployments with AWS Inferentia2 using TorchServe | PyTorch
    
  ](https://pytorch.org/blog/high-performance-llama/#:~:text=Inferentia2%20supports%20FP32%2C%20TF32%2C%20BF16%2C,removing%20the%20need%20for%20lower)). **Trainium** supports FP32 in software (e.g. accumulation or non-tensor ops), but for matrix ops it expects BF16/FP16; training in full FP32 would be very slow and is generally avoided ([Distributed Training of Large Language Models on AWS Trainium](https://assets.amazon.science/fa/fc/6c9a63824f1fa3655fc757825256/distributed-training-of-large-language-models-on-aws-trainium.pdf#:~:text=%E2%80%A2%20Tensor%20Engine%20is%20based,3%20TFLOPS%20of)) ([HLAT: High-quality Large Language Model Pre-trained on AWS Trainium](https://arxiv.org/html/2404.10630v1#:~:text=overall%20training%20throughput)).

## Training Performance and Estimated Training Cost

Training large LLMs is extremely resource-intensive – requiring running through hundreds of billions or even trillions of tokens. Below we compare training throughput (how many tokens per second can be processed) and the resulting time/cost to train model sizes from 1B to 70B parameters on different hardware.

**Throughput (tokens/sec) for training** is a function of hardware capability and model size. For example, AWS Trainium achieved **~35.8 sequences/sec (2048 tokens each)** on a 4-node Trn1 cluster for a 6.9B parameter model ([Frugality meets Accuracy: Cost-efficient training of GPT NeoX and Pythia models with AWS Trainium | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/frugality-meets-accuracy-cost-efficient-training-of-gpt-neox-and-pythia-models-with-aws-trainium/#:~:text=Model%20Tensor%20Parallel%20Pipeline%20Parallel,60%203%2C302%2C704)). This equates to ~73,000 tokens/sec across 4 Trn1.32xl instances (64 Trainium chips), or ~**10.1 million tokens processed per dollar** at that scale ([Frugality meets Accuracy: Cost-efficient training of GPT NeoX and Pythia models with AWS Trainium | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/frugality-meets-accuracy-cost-efficient-training-of-gpt-neox-and-pythia-models-with-aws-trainium/#:~:text=Model%20Tensor%20Parallel%20Pipeline%20Parallel,60%203%2C302%2C704)). In comparison, a similar 6-7B model on 4 GPU nodes would typically handle fewer tokens per second per dollar. In AWS’s experiment, a 20B model (NeoX 20B) on Trainium reached ~13.6 seq/sec (2048 seq length) on 4 Trn1 instances – about **3.3 million tokens per $** ([Frugality meets Accuracy: Cost-efficient training of GPT NeoX and Pythia models with AWS Trainium | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/frugality-meets-accuracy-cost-efficient-training-of-gpt-neox-and-pythia-models-with-aws-trainium/#:~:text=%28tokens%2F%24%29%20Pythia%206,60%203%2C302%2C704)). This cost-efficiency (~3.3M tokens per $) was roughly on par with or better than A100 GPUs. In fact, AWS notes that training GPT-NeoX-20B on Trainium achieved about **3.2M tokens per $**, with similar model accuracy as GPUs ([Frugality meets Accuracy: Cost-efficient training of GPT NeoX and Pythia models with AWS Trainium | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/frugality-meets-accuracy-cost-efficient-training-of-gpt-neox-and-pythia-models-with-aws-trainium/#:~:text=match%20at%20L282%20Trainium%20is,For%20additional)).

**Training cost estimates by model size:** To illustrate, consider the number of tokens required to train each model (assumed based on contemporary LLMs) and the cost on different hardware:

- **1B params** – Assume ~200 billion tokens. On an 8×A100 (40GB) instance, which provides ~500 TFLOPs effective throughput (half of 960 theoretical) ([[D] The cost of training GPT-3 : r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/hwfjej/d_the_cost_of_training_gpt3/#:~:text=In%20the%20paper%20there%20is,around%20500%20Teraflop%2Fs%20per%20machine)), this might take on the order of a few days. Estimated cost: *tens of thousands* of dollars. (For reference, GPT-2 (1.5B) trained on ~40GB of text – far fewer tokens – but modern 1B LLMs could use more if available.)

- **7B params** – Many 6–7B models are trained on ~1 trillion tokens or more. An academic example: a 7B model trained on **1.8 trillion tokens** on AWS Trainium used 64 Trn1 instances (1024 Trainium chips) and took on the order of **several days to a couple of weeks** ([HLAT: High-quality Large Language Model Pre-trained on AWS Trainium](https://arxiv.org/html/2404.10630v1#:~:text=We%20use%2064%20nodes%20with,8%20trillion%20tokens)). If that training run used 64× Trn1.32xl at ~$21.5/hr each, a 10-day run would cost ~$330k. In practice, HLAT (a 7B model on Trainium) indeed was trained to 1.8T tokens and “showcases” Trainium’s capability ([HLAT: High-quality Large Language Model Pre-trained on AWS Trainium](https://arxiv.org/html/2404.10630v1#:~:text=However%2C%20training%20LLMs%20with%20billions,HLAT%C2%A0is%20benchmarked%20against%20popular%20open)) – implying a cost on the order of **a few hundred thousand dollars** in AWS cloud. On 8×A100 nodes, the cost would be higher: using 64 p4d.24xl (8×A100) for the same duration would cost ~$704k (about **2× the Trainium cost** by rough comparison, consistent with Trainium’s ~50% cost-to-train savings ([AWS Inferentia and Trainium for Generative AI: A Comprehensive Guide - CloudThat Resources](https://www.cloudthat.com/resources/blog/aws-inferentia-and-trainium-for-generative-ai-a-comprehensive-guide/#:~:text=Training%20large%20models%2C%20especially%20those,vision%2C%20recommendation%20systems%2C%20and%20more))). Smaller setups or partial training would scale down proportionally. A single Trn1.32xl can process on the order of *millions of tokens per second*, so training 7B on one such instance (if it fit in memory) might still take months; hence distributed training is used.

- **13B params** – Assume ~1.5–2 trillion tokens. Using H100 GPUs could significantly speed this up. The H100 is ~2–3× faster than A100 for training in BF16 ([Benchmarking Large Language Models on NVIDIA H100 GPUs with ...](https://www.databricks.com/blog/coreweave-nvidia-h100-part-1#:~:text=Benchmarking%20Large%20Language%20Models%20on,We)). If an A100 cluster needed ~X weeks, an H100 cluster might do it in ~X/2 weeks. For cost: AWS on-demand for an H100 cluster is pricey ($98/hr for 8 H100s) but they also train faster. Azure’s H100 at ~$6.98/hr each ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=NC40ads%20H100%20v5)) might be a more cost-effective route. For a rough figure, if 13B with 2T tokens took ~15 days on 32 H100s, at ~$12.29/hr each on AWS that’s ~$141k. On 32 A100s (at ~$4/hr each), it might take 2–3× longer, perhaps ending up around ~$200–300k. Trainium could potentially handle 13B too (with more instances due to slightly lower per-chip FLOPs), likely still offering savings – e.g. 8 Trn1 instances (128 chips) could match performance of 8× H100, at ~60% the cost of GPUs. Exact numbers vary, but **Trainium is generally ~1.5–2× cost-efficient vs A100** ([AWS Inferentia and Trainium for Generative AI: A Comprehensive Guide - CloudThat Resources](https://www.cloudthat.com/resources/blog/aws-inferentia-and-trainium-for-generative-ai-a-comprehensive-guide/#:~:text=Training%20large%20models%2C%20especially%20those,vision%2C%20recommendation%20systems%2C%20and%20more)), and H100’s speed can offset its cost to some extent.

- **70B params** – High-end LLMs like LLaMA-2 70B were trained on ~2 trillion tokens. Using A100 80GB GPUs, Meta reportedly spent over a million dollar scale resources (exact figures not public). As a ballpark, training 70B (2T tokens) on **64 A100-80GB** GPUs might take several weeks and cost on the order of **$1–2 million**. AWS claims Trainium can handle 100B+ param models with up to 16 instances (256 chips) ([AWS Inferentia and Trainium for Generative AI: A Comprehensive Guide - CloudThat Resources](https://www.cloudthat.com/resources/blog/aws-inferentia-and-trainium-for-generative-ai-a-comprehensive-guide/#:~:text=Training%20large%20models%2C%20especially%20those,vision%2C%20recommendation%20systems%2C%20and%20more)), and would do so at half the cost of GPUs. Even so, we are talking many hundreds of thousands of dollars. For context, estimates for **GPT-3 (175B)** training ranged from ~$4.6M to $12M ([[D] The cost of training GPT-3 : r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/hwfjej/d_the_cost_of_training_gpt3/#:~:text=Discussion)) (OpenAI likely optimized costs with mixed precision and reserved capacity). One Reddit analysis calculated that using 64 V100 GPUs for ~4 months could train GPT-3 for around $1.9M (assuming ~50% utilization) ([[D] The cost of training GPT-3 : r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/hwfjej/d_the_cost_of_training_gpt3/#:~:text=In%20the%20paper%20there%20is,around%20500%20Teraflop%2Fs%20per%20machine)) ([[D] The cost of training GPT-3 : r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/hwfjej/d_the_cost_of_training_gpt3/#:~:text=Let%27s%20say%20we%20don%27t%20want,node%20communication)), though real-world inefficiencies push it higher. In summary, a **70B** model is a multi-million-token compute job likely costing **mid to high six figures** on Trainium or H100 clusters, and higher on older GPUs.

**Cost per training token:** Using the above, we can derive rough efficiency metrics. For example, the **cost per million tokens processed** on Trainium for a 6–20B model was measured in the single-digit cents: Pythia-6.9B on Trainium achieved ~8.7 million tokens per $ ([Frugality meets Accuracy: Cost-efficient training of GPT NeoX and Pythia models with AWS Trainium | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/frugality-meets-accuracy-cost-efficient-training-of-gpt-neox-and-pythia-models-with-aws-trainium/#:~:text=Model%20Tensor%20Parallel%20Pipeline%20Parallel,60%203%2C302%2C704)), i.e. ~$0.115 per million tokens. NeoX-20B was ~3.3M tokens per $, about $0.303 per million tokens ([Frugality meets Accuracy: Cost-efficient training of GPT NeoX and Pythia models with AWS Trainium | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/frugality-meets-accuracy-cost-efficient-training-of-gpt-neox-and-pythia-models-with-aws-trainium/#:~:text=Sequence%20length%20%20Global%20batch,60%203%2C302%2C704)). GPU figures are a bit higher; if A100 is ~50% less cost-efficient, it might be ~$0.6 per million for a 20B model. These are *per hardware dollar* metrics during training. In a full training run, total cost per *param* ends up a few cents – e.g. one source noted GPT-3’s $4.6M cost is about 2.6¢ per parameter (or $12M = 6.9¢/param). Smaller models have higher cost/param typically, as they still require a significant fixed token budget to learn adequately.

**Training time considerations:** All accelerators support half-precision which is essential for speed. Using FP32 for the entire training would more than double the cost/time and is rarely done. AWS notes that *“Pre-training with full precision (FP32) is inefficient for large LLMs”* and thus Trainium and GPUs use BF16/FP16 with mixed-precision techniques to maintain stability ([HLAT: High-quality Large Language Model Pre-trained on AWS Trainium](https://arxiv.org/html/2404.10630v1#:~:text=overall%20training%20throughput)). Techniques like gradient checkpointing, parallelism (data, tensor, pipeline), and high-bandwidth interconnects (NVLink, NVSwitch, NeuronLink) are employed on each platform to scale throughput linearly with more devices ([HLAT: High-quality Large Language Model Pre-trained on AWS Trainium](https://arxiv.org/html/2404.10630v1#:~:text=Since%20pre,often%20integrated%20with%20dedicated%20tensor)) ([HLAT: High-quality Large Language Model Pre-trained on AWS Trainium](https://arxiv.org/html/2404.10630v1#:~:text=accelerators%20across%20different%20machines%2C%20which,models%20with%20efficient%20hardware%20utilization)). For instance, a single H100 80GB has 3.35 TB/s memory bandwidth and NVLink4 at 900 GB/s total, enabling near-linear scaling to multi-GPU clusters for LLMs. Trainium chips are connected by 2× 800 Gbps NeuronLink cross-ships per instance and up to 800 Gbps network, also enabling efficient multi-node training ([Distributed Training of Large Language Models on AWS Trainium](https://assets.amazon.science/fa/fc/6c9a63824f1fa3655fc757825256/distributed-training-of-large-language-models-on-aws-trainium.pdf#:~:text=BF16%2FFP16%20TFLOPS%20190%20312%20Onboard,accelerator%20%28GB%2Fsec%2Faccelerator%29%20384%20600)).

**Bottom line (training):** AWS Trainium offers the **lowest training cost per FLOP** (roughly 50% of GPU cost) ([AWS Inferentia and Trainium for Generative AI: A Comprehensive Guide - CloudThat Resources](https://www.cloudthat.com/resources/blog/aws-inferentia-and-trainium-for-generative-ai-a-comprehensive-guide/#:~:text=Training%20large%20models%2C%20especially%20those,vision%2C%20recommendation%20systems%2C%20and%20more)), while NVIDIA H100 offers the **fastest training speed** per device (often 2–3× A100 throughput) ([H100 has 4.6x A100 Performance in TensorRT-LLM, achieving ...](https://nvidia.github.io/TensorRT-LLM/blogs/H100vsA100.html#:~:text=H100%20has%204,1st%20token%20latency%20than%20A100)), which can reduce wall-clock time and even yield lower cost at scale when opportunity cost is considered. NVIDIA A100 is a balanced workhorse with wide framework support, and A10G is suitable for smaller-scale or budget-conscious training (its cost is low, but its speed is much lower; best for “moderately complex models” that fit on a single node ([Amazon EC2 G5 Instances | Amazon Web Services](https://aws.amazon.com/ec2/instance-types/g5/#:~:text=Cost,ML%20models))). Training a multi-billion-parameter LLM will cost in the **hundreds of thousands to millions** range on any of these; choosing the right accelerator can save a significant fraction of that (e.g. **Trainium saved ~40%** on cost in AWS’s 20B model tests ([H100 GPU Instance Pricing On AWS: Grin And Bear It](https://www.nextplatform.com/2023/07/27/h100-gpu-instance-pricing-on-aws-grin-and-bear-it/#:~:text=Image)) ([H100 GPU Instance Pricing On AWS: Grin And Bear It](https://www.nextplatform.com/2023/07/27/h100-gpu-instance-pricing-on-aws-grin-and-bear-it/#:~:text=In%20its%20announcement%2C%20AWS%20said,premium%20for%20its%20A100%20GPU))).

## Inference Performance and Inference Cost

**Inference throughput and latency** are critical for deploying LLMs. Here the AWS Inferentia2 chips and NVIDIA’s high-end GPUs diverge in design: Inferentia2 is optimized purely for inference (int8/BF16), whereas GPUs can do both training and inference well, and Trainium is mainly for training but can serve for large-batch inference.

- **NVIDIA H100** currently offers the fastest single-chip inference for large models. Using FP8 quantization and Transformer Engine optimizations, an H100 can generate text at **up to 10k tokens per second** (for a 6B model at batch 64) while keeping first-token latency ~100ms ([H100 has 4.6x A100 Performance in TensorRT-LLM, achieving 10,000 tok/s at 100ms to first token — tensorrt_llm  documentation](https://nvidia.github.io/TensorRT-LLM/blogs/H100vsA100.html#:~:text=TensorRT,10ms%20to%201st%20token%20latency)). Even for larger models, H100 shows 4×+ speedups vs A100: e.g., running Llama-2 13B with TensorRT, H100 (FP8) was ~4.6× the throughput of A100 (FP16) ([H100 has 4.6x A100 Performance in TensorRT-LLM, achieving 10,000 tok/s at 100ms to first token — tensorrt_llm  documentation](https://nvidia.github.io/TensorRT-LLM/blogs/H100vsA100.html#:~:text=TensorRT,10ms%20to%201st%20token%20latency)). If an A100 produced ~1,700 tokens/sec for Llama2-13B (FP16), H100 delivered ~7,800 tok/s (FP8). This dramatically lowers the cost per query when using H100. **Latency** is also improved: at batch-1, H100 had ~4.4× lower latency for first token than A100 ([H100 has 4.6x A100 Performance in TensorRT-LLM, achieving 10,000 tok/s at 100ms to first token — tensorrt_llm  documentation](https://nvidia.github.io/TensorRT-LLM/blogs/H100vsA100.html#:~:text=TensorRT,10ms%20to%201st%20token%20latency)). These gains come from Hopper’s FP8 support and higher memory and interconnect bandwidth (important for sequence-heavy generation) ([H100 has 4.6x A100 Performance in TensorRT-LLM, achieving ...](https://nvidia.github.io/TensorRT-LLM/blogs/H100vsA100.html#:~:text=max%20throughput%20and%204,1st%20token%20latency%20than%20A100)).

- **NVIDIA A100** remains a strong inference performer, especially with optimizations like quantization (INT8). Many current deployments of GPT-3 class models use A100s. An 80GB A100 can serve a 70B model in memory. Throughput might reach on the order of ~2,000 tokens/sec for a 13B model with optimized code, and ~200–300 tokens/sec for a 70B model with batch-1 (latency of few seconds for a full response). The cost per inference on A100 is higher than on H100 or Inferentia2. For instance, AWS’s comparison shows Inferentia2 can be **3× cheaper** than “comparable GPU instances” for LLM inference ([
    
      High performance Llama 2 deployments with AWS Inferentia2 using TorchServe | PyTorch
    
  ](https://pytorch.org/blog/high-performance-llama/#:~:text=Cost)).

- **AWS Inferentia2 (Inf2)** is purpose-built to minimize **inference cost** while offering good throughput. An Inf2.48xlarge (with 12 Inferentia2 chips, 192 vCPUs) can achieve around **130 tokens/sec for Llama-2 7B** and **90 tokens/sec for Llama-2 13B** (at 256 input tokens, generating 256 output tokens) ([
    
      High performance Llama 2 deployments with AWS Inferentia2 using TorchServe | PyTorch
    
  ](https://pytorch.org/blog/high-performance-llama/#:~:text=Throughput)) ([
    
      High performance Llama 2 deployments with AWS Inferentia2 using TorchServe | PyTorch
    
  ](https://pytorch.org/blog/high-performance-llama/#:~:text=We%20now%20show%20the%20number,7B%20and%2013B%20models%2C%20respectively)). This is with full model parallelism across the 12 chips (24 NeuronCores). The end-to-end latency for 256 token output was only ~7.9 ms per token for 7B, and ~30.1 ms on a smaller Inf2.xlarge (1 chip) ([
    
      High performance Llama 2 deployments with AWS Inferentia2 using TorchServe | PyTorch
    
  ](https://pytorch.org/blog/high-performance-llama/#:~:text=We%20also%20compare%20the%20cost,xlarge)). Critically, **cost per inference** is extremely low: hosting Llama-2 7B on Inf2.48xl costs about **$0.011 per 1,000 tokens** (and $0.016 per 1k for 13B) using 3-year reserved pricing ([
    
      High performance Llama 2 deployments with AWS Inferentia2 using TorchServe | PyTorch
    
  ](https://pytorch.org/blog/high-performance-llama/#:~:text=Cost)). That translates to **$11 per million tokens** for 7B, which is *3× lower cost* than a GPU instance (i.e. roughly $33 per million on an equivalent GPU) ([
    
      High performance Llama 2 deployments with AWS Inferentia2 using TorchServe | PyTorch
    
  ](https://pytorch.org/blog/high-performance-llama/#:~:text=Cost)). Even on on-demand pricing, 7B might be around $0.02–$0.03 per 1k tokens on Inf2 (still well under $0.1/1k tokens that a GPU might cost). In practice, AWS customers have seen **up to 70% lower cost per inference** on Inferentia1 vs GPUs, and Inf2 improves on that further ([AWS Inferentia and Trainium for Generative AI: A Comprehensive Guide - CloudThat Resources](https://www.cloudthat.com/resources/blog/aws-inferentia-and-trainium-for-generative-ai-a-comprehensive-guide/#:~:text=AWS%20Inferentia%20is%20designed%20to,LLMs%29%20or%20diffusion%20models)). The trade-off is that each Inf2 chip has 32 GB memory, so serving very large models may require model partitioning across chips (Inf2 instances support up to 12 chips with fast interconnect). Inferentia2 supports FP8 and INT8 which can further boost throughput if the model is quantized, all while maintaining accuracy with techniques like autocasting ([
    
      High performance Llama 2 deployments with AWS Inferentia2 using TorchServe | PyTorch
    
  ](https://pytorch.org/blog/high-performance-llama/#:~:text=Inferentia2%20supports%20FP32%2C%20TF32%2C%20BF16%2C,removing%20the%20need%20for%20lower)).

- **AWS Trainium** can also be used for inference (especially batch inference). While not optimized for single-digit millisecond latency, a Trn1 with 16 chips and 512 GB total HBM can serve a 70B model in memory. Throughput in BF16 mode would be comparable to A100. However, Trainium lacks some of the transformer-specific optimizations that GPUs have for deployment, and the Neuron SDK is more tuned for training throughput. Still, if one has Trainium instances available, they could be used for offline inference or high-throughput scenarios. AWS has demonstrated serving models like RoBERTA on Inf2 vs Trn1, where Inf2 had the edge in cost and latency for inference ([AWS Inferentia and Trainium for Generative AI: A Comprehensive Guide - CloudThat Resources](https://www.cloudthat.com/resources/blog/aws-inferentia-and-trainium-for-generative-ai-a-comprehensive-guide/#:~:text=AWS%20Inferentia%20is%20designed%20to,LLMs%29%20or%20diffusion%20models)). Trainium’s value for inference would be in reuse of training hardware for inferring without needing GPUs.

- **NVIDIA A10G** is a lower-tier option for inference. With 24 GB memory and ~125 FP16 TFLOPS, an A10G can serve models up to ~13B (with model optimizations to fit memory) or larger models with 8-bit quantization. AWS G5 instances with A10G are promoted for “cost-efficient ML inference” with up to *40% better price-performance than previous gen (T4) instances* ([Amazon EC2 G5 Instances | Amazon Web Services](https://aws.amazon.com/ec2/instance-types/g5/#:~:text=High%20performance%20and%20cost,ML%20inference)). In practice, an A10G’s inference throughput might be roughly 1/4 of an A100’s on transformer models, but its cost is ~1/4 too, so it can be cost-effective. It lacks newer precision modes like FP8, but supports FP16 and INT8. For smaller models (a few billion params or less), A10G offers an inexpensive deployment path (roughly $1/hr vs $4/hr for A100) ([g5.xlarge pricing and specs - Vantage](https://instances.vantage.sh/aws/ec2/g5.xlarge#:~:text=Size%20vCPUs%20Memory%20%28GiB%29%20g5,48xlarge%20192%20768)) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=NC24ads%20A100%20v4)).

**Inference cost per query or per 1M tokens:** As a benchmark, using AWS’s Inf2 numbers: **$11 per million output tokens** for a 7B model ([
    
      High performance Llama 2 deployments with AWS Inferentia2 using TorchServe | PyTorch
    
  ](https://pytorch.org/blog/high-performance-llama/#:~:text=Cost)). If a typical user query returns 100 tokens, that’s ~$0.0011 – a tenth of a cent – on Inf2 (hardware cost only). A GPU might cost ~$0.003–$0.005 for the same 100-token output. These low costs assume the hardware is fully utilized. Real-world deployments might have overhead or under-utilization that raises effective cost. Nonetheless, these figures show that **hardware cost of LLM inference can be well under a penny per request** for reasonably sized models. Larger models (70B+) have slower throughput, so cost per token is higher. Using 8-bit quantization or model distillation can help. Notably, OpenAI’s pricing for GPT-4 (~$0.03 per 1k tokens) suggests they leverage highly optimized inference clusters to get close to these hardware cost levels (with margin included).

**Efficiency metrics:** A useful metric is **cost per 1 million inference tokens**. Summarizing from above: 

- AWS Inferentia2: ~$11 per 1M tokens (7B model) ([
    
      High performance Llama 2 deployments with AWS Inferentia2 using TorchServe | PyTorch
    
  ](https://pytorch.org/blog/high-performance-llama/#:~:text=Cost)).
- NVIDIA H100: If 7B on Inf2 is $11, and Inf2 is 3× cheaper, then H100 cluster might be on the order of $30 per 1M tokens for 7B. H100’s greater speed could narrow this gap if fully utilized.
- NVIDIA A100: likely ~$33–$40 per 1M for 7B (again rough, 3× Inf2).
- AWS Trainium: not typically used for real-time inference, but for batch generation one could achieve cost near to Inf2 for BF16 outputs. (Trainium’s cost per FLOP is low, but if not fully utilized for inference, the savings won’t be as high as for training.)
- NVIDIA A10G: Perhaps ~$20–$25 per 1M (a guess, as its price is low but also slower; if utilized at lower batch sizes, cost might increase).

In terms of **cost per TFLOP** for inference: Inferentia2 and Trainium lead (~$5–7 per hour for 1000 TFLOPS), followed by A10G (~$8/hr per 1000 TFLOPS), then H100 (~$12/hr/1000 TF) and A100 (~$13/hr/1000 TF). But actual inference efficiency depends more on memory and architecture than raw FLOPs.

**Key differences in precision support for inference:** H100’s FP8 confers a major advantage in throughput (with minimal quality loss on well-quantized models). Inferentia2’s **configurable FP8 (cFP8)** is similarly aimed at maximizing throughput, and it supports INT8 natively for deployment ([
    
      High performance Llama 2 deployments with AWS Inferentia2 using TorchServe | PyTorch
    
  ](https://pytorch.org/blog/high-performance-llama/#:~:text=Inferentia2%20supports%20FP32%2C%20TF32%2C%20BF16%2C,removing%20the%20need%20for%20lower)). A100 and Trainium do not support FP8, so they rely on INT8 for quantized speedups. A100’s tensor cores support INT8 and even 4-bit (via NVIDIA’s software INT4 techniques), but H100’s architecture is more geared to take advantage of lower precision. If an LLM can run in INT8 with calibration, all these platforms can benefit: e.g., an A100 can double throughput going from FP16 to INT8. Inferentia chips shine here because they were built with INT8 in mind from the start (Inferentia1 delivered 128 TOPS INT8 per chip ([[PDF] Deliver high performance ML inference with AWS Inferentia](https://d1.awsstatic.com/events/reinvent/2019/REPEAT_1_Deliver_high_performance_ML_inference_with_AWS_Inferentia_CMP324-R1.pdf#:~:text=,FP16%2C%20BF16%2C%20INT8%20data%20types))). In summary, **GPUs offer more flexibility** (you could even run FP32 inference for maximum fidelity, albeit slowly), whereas **Inferentia2 is designed to give optimal cost-performance at INT8/BF16**. That said, Inferentia2 can take in an FP32 model and automatically cast to BF16/INT8 for you ([
    
      High performance Llama 2 deployments with AWS Inferentia2 using TorchServe | PyTorch
    
  ](https://pytorch.org/blog/high-performance-llama/#:~:text=Inferentia2%20supports%20FP32%2C%20TF32%2C%20BF16%2C,removing%20the%20need%20for%20lower)) – so it effectively hides the precision complexity from the user.

## Summary Comparison

To wrap up, here is a high-level comparison of the accelerators for LLM workloads:

- **NVIDIA A10G:** Best for **budget-conscious tasks** and smaller models. Low hourly cost and decent FP16 performance (125 TFLOPS) make it efficient for inference of models up to ~13B or training moderate models. However, it lacks the raw muscle for fastest training of very large models. Supports FP32/FP16/INT8 but no FP8. Often used in single-node or single-GPU fine-tunes. *Cloud pricing:* ~$1/hr (AWS), ~$0.90/hr (Azure) ([g5.xlarge pricing and specs - Vantage](https://instances.vantage.sh/aws/ec2/g5.xlarge#:~:text=Size%20vCPUs%20Memory%20%28GiB%29%20g5,48xlarge%20192%20768)) ([NV12ads A10 v5 pricing and specs - Vantage](https://instances.vantage.sh/azure/vm/nv12ads-v5#:~:text=NV12ads%20A10%20v5)). *Training:* ~15% lower cost-to-train than prior-gen V100 instances ([Amazon EC2 G5 Instances | Amazon Web Services](https://aws.amazon.com/ec2/instance-types/g5/#:~:text=Cost,ML%20models)). *Inference:* Good price-performance (40% better price-performance than T4 GPU) ([Amazon EC2 G5 Instances | Amazon Web Services](https://aws.amazon.com/ec2/instance-types/g5/#:~:text=High%20performance%20and%20cost,ML%20inference)), but absolute throughput is lower than flagship GPUs.

- **NVIDIA A100:** A balanced choice for both **training and inference** of large models. Still widely used for 2020–2023 era LLMs. *Cloud cost:* ~$4–$6/hr per GPU on major clouds ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=NC24ads%20A100%20v4)) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=a2)). *Training:* 312 BF16 TFLOPS means an 8×A100 node offers ~2.5 PFLOPS – large clusters of A100s have trained models like GPT-3 (175B) in ~1–2 months at multi-million dollar cost ([[D] The cost of training GPT-3 : r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/hwfjej/d_the_cost_of_training_gpt3/#:~:text=Discussion)) ([[D] The cost of training GPT-3 : r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/hwfjej/d_the_cost_of_training_gpt3/#:~:text=As%20we%20know%20from%20the,9%20million)). *Inference:* Can serve models up to 80GB in one GPU. With optimizations, can achieve sub-second latency for medium models and a few seconds for 70B+. Cost per 1M tokens ~3× higher than Inferentia2 (around $30–$40) but still just a few dollars-digit range.

- **NVIDIA H100:** The **highest performer** – ideal for cutting-edge training (enabling multi-trillion token runs in feasible time) and fast inference with new precision tricks. *Cloud cost:* higher ($7–$12/hr) but with up to 6× training speedup over A100 ([H100 GPU Instance Pricing On AWS: Grin And Bear It](https://www.nextplatform.com/2023/07/27/h100-gpu-instance-pricing-on-aws-grin-and-bear-it/#:~:text=Image)) and ~4–5× inference throughput boost ([H100 has 4.6x A100 Performance in TensorRT-LLM, achieving 10,000 tok/s at 100ms to first token — tensorrt_llm  documentation](https://nvidia.github.io/TensorRT-LLM/blogs/H100vsA100.html#:~:text=TensorRT,10ms%20to%201st%20token%20latency)), it offers better *price/performance* in many cases (Azure notes H100’s price-performance is better than A100’s when fully utilized ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=Currently%20the%20H100%20%20is,69%20for%204xA100%20instances))). H100’s FP8 capability and larger memory bandwidth set it apart. It’s the go-to for organizations that need the *fastest* results and are willing to pay a premium upfront to save time (and potentially money at scale). Expect further improvements with upcoming NVIDIA generations (e.g. H200).

- **AWS Inferentia2:** The choice for **cost-effective inference at scale**. It slashes inference cost per query – AWS touts up to **70% cost reduction** vs GPU for Inf1 ([AWS Inferentia and Trainium for Generative AI: A Comprehensive Guide - CloudThat Resources](https://www.cloudthat.com/resources/blog/aws-inferentia-and-trainium-for-generative-ai-a-comprehensive-guide/#:~:text=AWS%20Inferentia%20is%20designed%20to,LLMs%29%20or%20diffusion%20models)), and Inf2 improves throughput 4× further. If your application involves serving many queries of an LLM (e.g. an AI assistant API), Inf2 instances can yield big savings. They do require model compilation to AWS Neuron and some adjustment (not all GPU ops directly translate, but frameworks like PyTorch and TensorFlow are supported via Neuron SDK). *Cloud cost:* ~$13/hr for a 12-chip, 192 vCPU Inf2.48xl that can serve multiple model replicas ([inf2.48xlarge pricing and specs - Vantage](https://instances.vantage.sh/aws/ec2/inf2.48xlarge#:~:text=inf2)). That instance can easily handle thousands of tokens/sec throughput. Smaller Inf2 sizes make it accessible to deploy even a single model cheaply. One limitation: **Inf2 is AWS-specific**, so one must use AWS infrastructure and tools. It also doesn’t support training (except perhaps minor fine-tuning) – it’s geared for inference only.

- **AWS Trainium (Trn1):** Designed for **high-efficiency training** of large models on AWS. It offers near-GPU performance at about **50% the cost** ([AWS Inferentia and Trainium for Generative AI: A Comprehensive Guide - CloudThat Resources](https://www.cloudthat.com/resources/blog/aws-inferentia-and-trainium-for-generative-ai-a-comprehensive-guide/#:~:text=Training%20large%20models%2C%20especially%20those,vision%2C%20recommendation%20systems%2C%20and%20more)), making it attractive for long training jobs. It integrates with AWS Neuron SDK; while maturity is improving, some models may need extra porting effort compared to NVIDIA’s well-established CUDA stack. For companies with very large training runs (billions of parameters over trillions of tokens), Trainium can save significant money. *Cloud cost:* ~$21.6/hr for 16 Trainium accelerators (trn1.32xlarge) ([Distributed Training of Large Language Models on AWS Trainium](https://assets.amazon.science/fa/fc/6c9a63824f1fa3655fc757825256/distributed-training-of-large-language-models-on-aws-trainium.pdf#:~:text=BF16%2FFP16%20TFLOPS%20190%20312%20Onboard,accelerator%20%28GB%2Fsec%2Faccelerator%29%20384%20600)), which is roughly $0.007 per BF16 TFLOP-hour – likely the best $/compute in the cloud for ML. With 16 accelerators and 512 GB total HBM, a single Trn1 instance can handle substantial workloads; multi-instance clusters handle the largest models. One downside: Trainium is only on AWS and currently one generation old relative to H100 – a **Trainium2** is expected and is rumored to target **2× H100 performance** in the future ([How AWS Can Undercut Nvidia With Homegrown AI Compute ...](https://www.nextplatform.com/2023/12/04/how-aws-can-undercut-nvidia-with-homegrown-ai-compute-engines/#:~:text=How%20AWS%20Can%20Undercut%20Nvidia,H200%20that%20Nvidia%20just)), which could make the next-gen Trainium even more dominant in cost/performance. 

**Final takeaways:** Each accelerator has a niche:

- For *maximum speed* (minimal training time or highest inference throughput): **NVIDIA H100** leads, albeit at higher per-hour cost.
- For *minimum cost* to train a given model: **AWS Trainium** can cut the bill roughly in half compared to GPUs ([AWS Inferentia and Trainium for Generative AI: A Comprehensive Guide - CloudThat Resources](https://www.cloudthat.com/resources/blog/aws-inferentia-and-trainium-for-generative-ai-a-comprehensive-guide/#:~:text=Training%20large%20models%2C%20especially%20those,vision%2C%20recommendation%20systems%2C%20and%20more)).
- For *minimum cost* to serve a model: **AWS Inferentia2** provides the lowest cost per million inference tokens ([
    
      High performance Llama 2 deployments with AWS Inferentia2 using TorchServe | PyTorch
    
  ](https://pytorch.org/blog/high-performance-llama/#:~:text=Cost)), making it ideal for production endpoints where every fraction of a cent matters.
- **NVIDIA A100** remains a strong general-purpose baseline – if you need a multi-cloud or on-prem solution, A100s are ubiquitous and well-supported, though not as cheap or fast as the newer options.
- **NVIDIA A10G** offers a lower-cost entry point for smaller-scale needs, and can be a sweet spot for single-node tasks or inference on models that fit within 24 GB memory, delivering good bang-for-buck on those workloads.

Overall, cloud providers are aligning instance offerings to these strengths: AWS with its custom silicon for cost-efficiency, and Azure/GCP offering the latest NVIDIA GPUs for raw performance. Users training or deploying LLMs should choose hardware based on whether **time-to-result or cost-to-result** is the priority – fortunately, the data above shows options that excel at each, and even a mix (e.g. one might train on Trainium, then serve on Inferentia2, if the model can be exported to AWS Neuron format). The rapid advances in both hardware (e.g. FP8 support, high-bandwidth memory) and cloud-scale deployments are driving down the effective cost per model, bringing previously “million-dollar” LLM training tasks into a more accessible range and making large-model inference economically viable at scale.

**Sources:** Official cloud documentation and benchmarks were used wherever possible. For pricing, see AWS, Azure, GCP pricing pages ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=The%20H100%20%20is%20currently,for%20a%208x%20GPU%20instance)) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=NC40ads%20H100%20v5)) ([Cloud GPU Pricing Comparison in 2025 — Blog — DataCrunch](https://datacrunch.io/blog/cloud-gpu-pricing-comparison#:~:text=a3)). Performance metrics are drawn from NVIDIA specs ([Theoretical TFLOPS for FP16, BF16 and TF32 for tensor and non-tensor - GPU-Accelerated Libraries - NVIDIA Developer Forums](https://forums.developer.nvidia.com/t/theoretical-tflops-for-fp16-bf16-and-tf32-for-tensor-and-non-tensor/218102#:~:text=GPU%20Features%20NVIDIA%20A100%20NVIDIA,Tensor%29%20%7C78%7C120%7C96)), AWS blog posts ([Frugality meets Accuracy: Cost-efficient training of GPT NeoX and Pythia models with AWS Trainium | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/frugality-meets-accuracy-cost-efficient-training-of-gpt-neox-and-pythia-models-with-aws-trainium/#:~:text=Model%20Tensor%20Parallel%20Pipeline%20Parallel,60%203%2C302%2C704)) ([
    
      High performance Llama 2 deployments with AWS Inferentia2 using TorchServe | PyTorch
    
  ](https://pytorch.org/blog/high-performance-llama/#:~:text=Throughput)), and benchmark reports (AWS/Neuron, NVIDIA TensorRT) ([H100 has 4.6x A100 Performance in TensorRT-LLM, achieving 10,000 tok/s at 100ms to first token — tensorrt_llm  documentation](https://nvidia.github.io/TensorRT-LLM/blogs/H100vsA100.html#:~:text=TensorRT,10ms%20to%201st%20token%20latency)). AWS’s claims of 50% cost reduction with Trainium and 70% with Inferentia are noted in their blog and third-party analyses ([AWS Inferentia and Trainium for Generative AI: A Comprehensive Guide - CloudThat Resources](https://www.cloudthat.com/resources/blog/aws-inferentia-and-trainium-for-generative-ai-a-comprehensive-guide/#:~:text=Training%20large%20models%2C%20especially%20those,vision%2C%20recommendation%20systems%2C%20and%20more)) ([AWS Inferentia and Trainium for Generative AI: A Comprehensive Guide - CloudThat Resources](https://www.cloudthat.com/resources/blog/aws-inferentia-and-trainium-for-generative-ai-a-comprehensive-guide/#:~:text=AWS%20Inferentia%20is%20designed%20to,LLMs%29%20or%20diffusion%20models)). Large-scale cost estimates (GPT-3 etc.) are based on community calculations ([[D] The cost of training GPT-3 : r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/hwfjej/d_the_cost_of_training_gpt3/#:~:text=Discussion)) and should be seen as illustrative of scale.
